{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "948FefTIxPlC"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
    "from skimage import io, transform\n",
    "from torchvision import transforms, utils\n",
    "import torch.optim as optim\n",
    "import time\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/\"\n",
    "whole_data = pickle.load(open(\"whole_data\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(seed=1)\n",
    "\n",
    "train_imgs = np.loadtxt(data_path + \"referit_train_imlist.txt\").astype(\"int\")\n",
    "valid_imgs = np.loadtxt(data_path + \"referit_val_imlist.txt\").astype(\"int\")\n",
    "test_imgs = np.loadtxt(data_path + \"referit_test_imlist.txt\").astype(\"int\")\n",
    "\n",
    "train_ids = np.isin(whole_data[\"img\"].values.astype(\"int\"), train_imgs)\n",
    "tr_ids = list(np.argwhere(train_ids==True))\n",
    "tr_ids = [item for sublist in tr_ids for item in sublist]\n",
    "tr_ids = np.array(tr_ids)\n",
    "all_tr_ids = tr_ids\n",
    "tr_ids = np.random.choice(tr_ids, 64000, replace = False)\n",
    "#tr_ids = np.random.choice(tr_ids, 64, replace = False)\n",
    "\n",
    "valid_ids = np.isin(whole_data[\"img\"].values.astype(\"int\"), valid_imgs)\n",
    "vld_ids = list(np.argwhere(valid_ids==True))\n",
    "vld_ids = [item for sublist in vld_ids for item in sublist]\n",
    "vld_ids = np.array(vld_ids)\n",
    "\n",
    "vld_ids = np.random.choice(vld_ids, 6400, replace = False)\n",
    "#vld_ids = np.random.choice(vld_ids, 32, replace = False)\n",
    "\n",
    "test_ids = np.isin(whole_data[\"img\"].values.astype(\"int\"), test_imgs)\n",
    "tst_ids = list(np.argwhere(test_ids==True))\n",
    "tst_ids = [item for sublist in tst_ids for item in sublist]\n",
    "tst_ids = np.array(tst_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "595444"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.isin(whole_data[\"img\"].values.astype(\"int\"), train_imgs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.23376325211488436"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mean iou of dataset\n",
    "(whole_data.iloc[vld_ids]['IoU']-whole_data.iloc[tr_ids]['IoU'].mean()).abs().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the model \n",
    "import model_v3dot3 as modelClass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = modelClass.LocalizationDataset(data_pickle=whole_data,\n",
    "                                   data_path=data_path,\n",
    "                                   transform=transforms.Compose([\n",
    "                                               modelClass.Rescale((224,224), (224,224)),\n",
    "                                               modelClass.ToTensor()\n",
    "                                           ]))\n",
    "\n",
    "train_sampler = SubsetRandomSampler(all_tr_ids)\n",
    "valid_sampler = SubsetRandomSampler(vld_ids)\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, \n",
    "                                           sampler=train_sampler, num_workers=8)\n",
    "validation_loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size,\n",
    "                                                sampler=valid_sampler, num_workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21307067824787698"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_data.iloc[tr_ids]['IoU'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.09504117310886573"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whole_data.iloc[vld_ids]['IoU'].var()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fr2G6p6FaDB1"
   },
   "outputs": [],
   "source": [
    "device = 'cuda'\n",
    "\n",
    "model = modelClass.myModel()\n",
    "model.to(device)\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.05, momentum=0.9)\n",
    "train_loss = []\n",
    "val_loss = []\n",
    "\n",
    "load = True\n",
    "load_path = 'models/model_v3.3_full_data_iter66000.pt'\n",
    "\n",
    "if load:\n",
    "    checkpoint = torch.load(load_path)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    train_loss = checkpoint['train_loss']\n",
    "    val_loss = checkpoint['validation_loss']\n",
    "\n",
    "touch_vgg = True\n",
    "requires_grad = True\n",
    "if touch_vgg:\n",
    "    for child in model.img_feature_extractor.children():\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "\n",
    "    for child in model.box_feature_extractor.children():\n",
    "        for param in child.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18608"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c-qsKr1-0MUk"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "at epoch: 5\n"
     ]
    }
   ],
   "source": [
    "max_epochs = 10\n",
    "pr_freq = 500\n",
    "save_freq = 2000\n",
    "start_epoch = 0\n",
    "\n",
    "if load:\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "\n",
    "i=66000\n",
    "for epoch in range(start_epoch, max_epochs):\n",
    "    # Training\n",
    "    print('at epoch: ' + str(epoch))\n",
    "    tick_epoch = time.time()\n",
    "    tick_pr = time.time()\n",
    "    for sample in train_loader:        \n",
    "        i+=1\n",
    "        \n",
    "        image, bbox_image, loc_rel, embedding, IoU = modelClass.get_torch_data(sample)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(image,bbox_image,loc_rel,embedding)\n",
    "        loss = modelClass.my_loss(outputs, IoU)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss.append(loss.item())\n",
    "        # Validation\n",
    "        if(i%pr_freq == 0):\n",
    "            tock_pr = time.time()\n",
    "            print(str(pr_freq) + ' batch in ' + str((tock_pr-tick_pr)/60) + ' minutes')\n",
    "            with torch.set_grad_enabled(False):\n",
    "                l = []\n",
    "                for sample in validation_loader:\n",
    "                    image, bbox_image, loc_rel, embedding, IoU = modelClass.get_torch_data(sample)\n",
    "                    outputs = model(image,bbox_image,loc_rel,embedding)\n",
    "                    loss = modelClass.my_loss(outputs, IoU, margin= 0.004)\n",
    "                    l.append(loss.item())\n",
    "                val_l = np.average(l) \n",
    "                print('validation loss at iter ' + str(i) + ': ' + str(val_l))\n",
    "                val_loss.append((i,val_l))\n",
    "                print('train loss at iter ' + str(i) +': '+ str(np.average(train_loss[-pr_freq:])))\n",
    "            tick_pr = tock_pr\n",
    "        if(i%save_freq == 0):\n",
    "            torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'validation_loss': val_loss\n",
    "            }, 'models/model_v3.3_full_data_iter' + str(i) + '.pt')\n",
    "    tock_epoch = time.time()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    print('epoch in ' + str((tock_epoch - tick_epoch)/60) + ' minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 424,
     "status": "ok",
     "timestamp": 1559219227658,
     "user": {
      "displayName": "Hakan Åžirin",
      "photoUrl": "",
      "userId": "09673662114943124220"
     },
     "user_tz": -120
    },
    "id": "nZDnQLodPRRD",
    "outputId": "88f34f35-5588-4b0c-e9cc-0f6d254b5fb8"
   },
   "outputs": [],
   "source": [
    "torch.cuda.memory_allocated(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 416,
     "status": "ok",
     "timestamp": 1559214779547,
     "user": {
      "displayName": "Hakan Åžirin",
      "photoUrl": "",
      "userId": "09673662114943124220"
     },
     "user_tz": -120
    },
    "id": "jdqa243XT8qB",
    "outputId": "f93d2641-f6bb-478c-857f-0870bc113a7f"
   },
   "outputs": [],
   "source": [
    "torch.cuda.max_memory_allocated(device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load = True\n",
    "load_path = 'models/model18.pt'\n",
    "if load:\n",
    "    checkpoint = torch.load(load_path)\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    train_loss = checkpoint['train_loss']\n",
    "    val_loss = checkpoint['validation_loss']\n",
    "i = 0\n",
    "ground_truth_error = []\n",
    "false_box_error = []\n",
    "with torch.set_grad_enabled(False):\n",
    "    for sample in validation_loader:\n",
    "        image, bbox_image, loc_rel, embedding, IoU = get_torch_data(sample)\n",
    "        outputs = model(image,bbox_image,loc_rel,embedding)\n",
    "        break\n",
    "        img_seg, word_seg = torch.split(outputs, 512, dim=1)\n",
    "        cos = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "        cos_sim = cos(img_seg, word_seg)\n",
    "        dist = (1-cos_sim)/2\n",
    "        dist = dist.cpu().numpy()\n",
    "        IoU = IoU.cpu().numpy()\n",
    "        ground_truth_error.append(list(dist[IoU==1]))\n",
    "        false_box_error.append(list(dist[IoU==0]))\n",
    "print('DONE!!')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth_error = [item for sublist in ground_truth_error for item in sublist]\n",
    "false_box_error = [item for sublist in false_box_error for item in sublist]\n",
    "print(np.average(ground_truth_error))\n",
    "print(np.std(ground_truth_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.average(false_box_error))\n",
    "print(np.std(false_box_error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hiddenlayer as hl\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hl_graph = hl.build_graph(model, img, transforms = transforms)\n",
    "hl_graph.theme = hl.graph.THEMES[\"blue\"].copy()\n",
    "hl_graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "20.22411298751831 / 32 \n",
    "\n",
    "10.583017110824585 / 16\n",
    "\n",
    "8.651746273040771 / 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "img_shape = torch.Size((batch_size, 3, 240, 240))\n",
    "img = torch.cuda.FloatTensor(img_shape)\n",
    "\n",
    "box_shape = torch.Size((batch_size, 3, 240, 240))\n",
    "box = torch.cuda.FloatTensor()\n",
    "\n",
    "box_data_shape = torch.Size((batch_size, 4))\n",
    "box_data = torch.cuda.FloatTensor(box_data_shape)\n",
    "\n",
    "embeddings_shape = torch.Size((batch_size, 512))\n",
    "embedding = torch.cuda.FloatTensor(embeddings_shape)\n",
    "\n",
    "torch.randn(img_shape, out=img)\n",
    "torch.randn(box_shape, out=box)\n",
    "torch.randn(box_data_shape, out=box_data)\n",
    "torch.randn(embeddings_shape, out=embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "whole_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "img_shape = torch.Size((batch_size, 512, 7, 7))\n",
    "img = torch.randn(img_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1 = nn.Conv2d(512, 128, 3, stride=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 128, 3, 3])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv1(img).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace)\n",
      "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace)\n",
      "Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace)\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace)\n",
      "Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace)\n",
      "Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "ReLU(inplace)\n",
      "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n"
     ]
    }
   ],
   "source": [
    "for child in model.img_feature_extractor.children():\n",
    "    print(child)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for child in model.img_feature_extractor.children():\n",
    "    for param in child.parameters():\n",
    "        param.requires_grad = not freeze_vgg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "model.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
